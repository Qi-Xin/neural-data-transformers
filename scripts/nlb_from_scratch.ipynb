{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","- Let's setup NDT for train/eval from scratch.\n","- (Assumes your device is reasonably pytorch/GPU compatible)\n","- This is an interactive python script run via vscode. If you'd like to run as a notebook\n","\n","Run to setup requirements:\n","```\n","    Making a new env\n","    - python 3.7\n","    - conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n","    - conda install seaborn\n","    - pip install yacs pytorch_transformers tensorboard \"ray[tune]\" sklearn\n","    - pip install dandi \"pynwb>=2.0.0\"\n","    Or from environment.yaml\n","\n","    Then,\n","    - conda develop ~/path/to/nlb_tools\n","```\n","\n","Install NLB dataset(s), and create the h5s for training. (Here we install MC_Maze_Small)\n","```\n","    pip install dandi\n","    dandi download DANDI:000140/0.220113.0408\n","```\n","\n","This largely follows the `basic_example.ipynb` in `nlb_tools`. The only distinction is that we save to an h5.\n","\"\"\"\n","\n","from nlb_tools.nwb_interface import NWBDataset\n","from nlb_tools.make_tensors import (\n","    make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",")\n","from nlb_tools.evaluation import evaluate\n","\n","import numpy as np\n","import pandas as pd\n","import h5py\n","\n","import logging\n","logging.basicConfig(level=logging.INFO)\n","\n","dataset_name = 'mc_maze_small'\n","datapath = '000140/sub-Jenkins/'\n","dataset = NWBDataset(datapath)\n","\n","# Prepare dataset\n","phase = 'val'\n","\n","# Choose bin width and resample\n","bin_width = 5\n","dataset.resample(bin_width)\n","\n","# Create suffix for group naming later\n","suffix = '' if (bin_width == 5) else f'_{int(bin_width)}'\n","\n","train_split = 'train' if (phase == 'val') else ['train', 'val']\n","train_dict = make_train_input_tensors(\n","    dataset, dataset_name=dataset_name, trial_split=train_split, save_file=False,\n","    include_behavior=True,\n","    include_forward_pred = True,\n",")\n","\n","# Show fields of returned dict\n","print(train_dict.keys())\n","\n","# Unpack data\n","train_spikes_heldin = train_dict['train_spikes_heldin']\n","train_spikes_heldout = train_dict['train_spikes_heldout']\n","\n","# Print 3d array shape: trials x time x channel\n","print(train_spikes_heldin.shape)\n","\n","## Make eval data (i.e. val)\n","\n","# Split for evaluation is same as phase name\n","eval_split = phase\n","# Make data tensors\n","eval_dict = make_eval_input_tensors(\n","    dataset, dataset_name=dataset_name, trial_split=eval_split, save_file=False,\n",")\n","print(eval_dict.keys()) # only includes 'eval_spikes_heldout' if available\n","eval_spikes_heldin = eval_dict['eval_spikes_heldin']\n","\n","print(eval_spikes_heldin.shape)\n","\n","h5_dict = {\n","    **train_dict,\n","    **eval_dict\n","}\n","\n","h5_target = '/home/joelye/user_data/nlb/mc_maze_small.h5'\n","save_to_h5(h5_dict, h5_target)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","- At this point we should be able to train a basic model.\n","- In CLI, run a training call, replacing the appropriate paths\n","```\n","    ./scripts/train.sh mc_maze_small_from_scratch\n","\n","    OR\n","\n","    python ray_random.py -e ./configs/mc_maze_small_from_scratch.yaml\n","    (CLI overrides aren't available here, so make another config file)\n","```\n","- Once this is done training (~0.5hr for non-search), let's load the results...\n","\"\"\"\n","import os\n","import os.path as osp\n","from pathlib import Path\n","import sys\n","\n","# Add ndt src if not in path\n","module_path = osp.abspath(osp.join('..'))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","\n","import time\n","import h5py\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import torch\n","import torch.nn.functional as f\n","from torch.utils import data\n","\n","from nlb_tools.evaluation import evaluate\n","\n","from src.run import prepare_config\n","from src.runner import Runner\n","from src.dataset import SpikesDataset, DATASET_MODES\n","\n","from analyze_utils import init_by_ckpt\n","\n","variant = \"mc_maze_small_from_scratch\"\n","\n","is_ray = True\n","is_ray = False\n","\n","if is_ray:\n","    ckpt_path = f\"/home/joelye/user_data/nlb/ndt_runs/ray/{variant}/best_model/ckpts/{variant}.lve.pth\"\n","    runner, spikes, rates, heldout_spikes, forward_spikes = init_by_ckpt(ckpt_path, mode=DATASET_MODES.val)\n","else:\n","    ckpt_dir = Path(\"/home/joelye/user_data/nlb/ndt_runs/\")\n","    ckpt_path = ckpt_dir / variant / f\"{variant}.lve.pth\"\n","    runner, spikes, rates, heldout_spikes, forward_spikes = init_by_ckpt(ckpt_path, mode=DATASET_MODES.val)\n","\n","eval_rates, _ = runner.get_rates(\n","    checkpoint_path=ckpt_path,\n","    save_path = None,\n","    mode = DATASET_MODES.val\n",")\n","train_rates, _ = runner.get_rates(\n","    checkpoint_path=ckpt_path,\n","    save_path = None,\n","    mode = DATASET_MODES.train\n",")\n","\n","# * Val\n","eval_rates, eval_rates_forward = torch.split(eval_rates, [spikes.size(1), eval_rates.size(1) - spikes.size(1)], 1)\n","eval_rates_heldin_forward, eval_rates_heldout_forward = torch.split(eval_rates_forward, [spikes.size(-1), heldout_spikes.size(-1)], -1)\n","train_rates, _ = torch.split(train_rates, [spikes.size(1), train_rates.size(1) - spikes.size(1)], 1)\n","eval_rates_heldin, eval_rates_heldout = torch.split(eval_rates, [spikes.size(-1), heldout_spikes.size(-1)], -1)\n","train_rates_heldin, train_rates_heldout = torch.split(train_rates, [spikes.size(-1), heldout_spikes.size(-1)], -1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# * Viz some trials\n","trials = [1, 2, 3]\n","neuron = 10\n","trial_rates = train_rates_heldout[trials, :, neuron].cpu()\n","trial_spikes = heldout_spikes[trials, :, neuron].cpu()\n","trial_time = heldout_spikes.size(1)\n","\n","spike_level = 0.05\n","f, axes = plt.subplots(2, figsize=(6, 4))\n","times = np.arange(0, trial_time * 0.05, 0.05)\n","for trial_index in range(trial_spikes.size(0)):\n","    spike_times, = np.where(trial_spikes[trial_index].numpy())\n","    spike_times = spike_times * 0.05\n","    axes[0].scatter(spike_times, spike_level * (trial_index + 1)*np.ones_like(spike_times), marker='|', label='Spikes', s=30)\n","    axes[1].plot(times, trial_rates[trial_index].exp())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Submission e.g. as in `basic_example.ipynb`\n","# Looks like this model is pretty terrible :/\n","output_dict = {\n","    dataset_name + suffix: {\n","        'train_rates_heldin': train_rates_heldin.cpu().numpy(),\n","        'train_rates_heldout': train_rates_heldout.cpu().numpy(),\n","        'eval_rates_heldin': eval_rates_heldin.cpu().numpy(),\n","        'eval_rates_heldout': eval_rates_heldout.cpu().numpy(),\n","        'eval_rates_heldin_forward': eval_rates_heldin_forward.cpu().numpy(),\n","        'eval_rates_heldout_forward': eval_rates_heldout_forward.cpu().numpy()\n","    }\n","}\n","\n","# Reset logging level to hide excessive info messages\n","logging.getLogger().setLevel(logging.WARNING)\n","\n","# If 'val' phase, make the target data\n","if phase == 'val':\n","    # Note that the RTT task is not well suited to trial averaging, so PSTHs are not made for it\n","    target_dict = make_eval_target_tensors(dataset, dataset_name=dataset_name, train_trial_split='train', eval_trial_split='val', include_psth=True, save_file=False)\n","\n","    # Demonstrate target_dict structure\n","    print(target_dict.keys())\n","    print(target_dict[dataset_name + suffix].keys())\n","\n","# Set logging level again\n","logging.getLogger().setLevel(logging.INFO)\n","\n","if phase == 'val':\n","    print(evaluate(target_dict, output_dict))\n","\n","# e.g. with targets to compare to\n","# target_dict = torch.load(f'/snel/home/joely/tmp/{variant}_target.pth')\n","# target_dict = np.load(f'/snel/home/joely/tmp/{variant}_target.npy', allow_pickle=True).item()\n","\n","# print(evaluate(target_dict, output_dict))\n","\n","# e.g. to upload to EvalAI\n","# with h5py.File('ndt_maze_preds.h5', 'w') as f:\n","#     group = f.create_group('mc_maze')\n","#     for key in output_dict['mc_maze']:\n","#         group.create_dataset(key, data=output_dict['mc_maze'][key])"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
