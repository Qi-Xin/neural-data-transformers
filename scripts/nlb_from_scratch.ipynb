{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","%matplotlib inline"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:nlb_tools.nwb_interface:Loading /home/qix/user_data/000140/sub-Jenkins/sub-Jenkins_ses-small_desc-test_ecephys.nwb\n","/home/qix/anaconda3/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n","  return func(args[0], **pargs)\n","/home/qix/anaconda3/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.6.0-alpha is already loaded.\n","  return func(args[0], **pargs)\n","/home/qix/anaconda3/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n","  return func(args[0], **pargs)\n","INFO:nlb_tools.nwb_interface:Loading /home/qix/user_data/000140/sub-Jenkins/sub-Jenkins_ses-small_desc-train_behavior+ecephys.nwb\n","/home/qix/anaconda3/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n","  return func(args[0], **pargs)\n","/home/qix/anaconda3/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.6.0-alpha is already loaded.\n","  return func(args[0], **pargs)\n","/home/qix/anaconda3/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n","  return func(args[0], **pargs)\n","INFO:nlb_tools.nwb_interface:Resampling data to 5 ms.\n","INFO:nlb_tools.nwb_interface:Aligned 75 trials to move_onset_time with offset of (-250, 450) ms and margin of 0.\n","INFO:nlb_tools.nwb_interface:Aligned 25 trials to move_onset_time with offset of (-250, 450) ms and margin of 0.\n","INFO:nlb_tools.make_tensors:Saved data to /home/qix/user_data/nlb/mc_maze_small.h5\n"]},{"name":"stdout","output_type":"stream","text":["dict_keys(['train_spikes_heldin', 'train_spikes_heldout'])\n","(74, 140, 107)\n","dict_keys(['eval_spikes_heldin', 'eval_spikes_heldout'])\n","(25, 140, 107)\n"]}],"source":["\"\"\"\n","- Let's setup NDT for train/eval from scratch.\n","- (Assumes your device is reasonably pytorch/GPU compatible)\n","- This is an interactive python script run via vscode. If you'd like to run as a notebook\n","\n","Run to setup requirements:\n","```\n","    Making a new env\n","    - python 3.7\n","    - conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n","    - conda install seaborn\n","    - pip install yacs pytorch_transformers tensorboard \"ray[tune]\" sklearn\n","    - pip install dandi \"pynwb>=2.0.0\"\n","    Or from environment.yaml\n","\n","    Then,\n","    - conda develop ~/path/to/nlb_tools\n","```\n","\n","Install NLB dataset(s), and create the h5s for training. (Here we install MC_Maze_Small)\n","```\n","    pip install dandi\n","    dandi download DANDI:000140/0.220113.0408\n","```\n","\n","This largely follows the `basic_example.ipynb` in `nlb_tools`. The only distinction is that we save to an h5.\n","\"\"\"\n","import os\n","os.chdir('/home/qix/neural-data-transformers')\n","\n","from nlb_tools.nwb_interface import NWBDataset\n","from nlb_tools.make_tensors import (\n","    make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",")\n","from nlb_tools.evaluation import evaluate\n","\n","import numpy as np\n","import pandas as pd\n","import h5py\n","\n","import logging\n","logging.basicConfig(level=logging.INFO)\n","\n","dataset_name = 'mc_maze_small'\n","datapath = '/home/qix/user_data/000140/sub-Jenkins/'\n","dataset = NWBDataset(datapath)\n","\n","# Prepare dataset\n","phase = 'val'\n","\n","# Choose bin width and resample\n","bin_width = 5\n","dataset.resample(bin_width)\n","\n","# Create suffix for group naming later\n","suffix = '' if (bin_width == 5) else f'_{int(bin_width)}'\n","\n","train_split = 'train' if (phase == 'val') else ['train', 'val']\n","train_dict = make_train_input_tensors(dataset, dataset_name=dataset_name, trial_split=train_split, save_file=False)\n","\n","# Show fields of returned dict\n","print(train_dict.keys())\n","\n","# Unpack data\n","train_spikes_heldin = train_dict['train_spikes_heldin']\n","train_spikes_heldout = train_dict['train_spikes_heldout']\n","\n","# Print 3d array shape: trials x time x channel\n","print(train_spikes_heldin.shape)\n","\n","## Make eval data (i.e. val)\n","\n","# Split for evaluation is same as phase name\n","eval_split = phase\n","# Make data tensors\n","eval_dict = make_eval_input_tensors(\n","    dataset, dataset_name=dataset_name, trial_split=eval_split, save_file=False,\n",")\n","print(eval_dict.keys()) # only includes 'eval_spikes_heldout' if available\n","eval_spikes_heldin = eval_dict['eval_spikes_heldin']\n","\n","print(eval_spikes_heldin.shape)\n","\n","h5_dict = {\n","    **train_dict,\n","    **eval_dict\n","}\n","\n","h5_target = '/home/qix/user_data/nlb/mc_maze_small.h5'\n","save_to_h5(h5_dict, h5_target)\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/home/qix/user_data/nlb/ndt_runs/mc_maze_small_from_scratch/mc_maze_small_from_scratch.lve.pth'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m     ckpt_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/qix/user_data/nlb/ndt_runs/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_dir \u001b[38;5;241m/\u001b[39m variant \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.lve.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m     runner, spikes, rates, heldout_spikes, forward_spikes \u001b[38;5;241m=\u001b[39m \u001b[43minit_by_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_MODES\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m eval_rates, _ \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39mget_rates(\n\u001b[1;32m     55\u001b[0m     checkpoint_path\u001b[38;5;241m=\u001b[39mckpt_path,\n\u001b[1;32m     56\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m     mode \u001b[38;5;241m=\u001b[39m DATASET_MODES\u001b[38;5;241m.\u001b[39mval\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     59\u001b[0m train_rates, _ \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39mget_rates(\n\u001b[1;32m     60\u001b[0m     checkpoint_path\u001b[38;5;241m=\u001b[39mckpt_path,\n\u001b[1;32m     61\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m     mode \u001b[38;5;241m=\u001b[39m DATASET_MODES\u001b[38;5;241m.\u001b[39mtrain\n\u001b[1;32m     63\u001b[0m )\n","File \u001b[0;32m~/neural-data-transformers/scripts/analyze_utils.py:49\u001b[0m, in \u001b[0;36minit_by_ckpt\u001b[0;34m(ckpt_path, mode)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_by_ckpt\u001b[39m(ckpt_path, mode\u001b[38;5;241m=\u001b[39mDATASET_MODES\u001b[38;5;241m.\u001b[39mval):\n\u001b[0;32m---> 49\u001b[0m     runner \u001b[38;5;241m=\u001b[39m \u001b[43mRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     runner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     51\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m~/neural-data-transformers/src/runner.py:76\u001b[0m, in \u001b[0;36mRunner.__init__\u001b[0;34m(self, config, checkpoint_path)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrolling_metrics \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# For PBT\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     ckpt_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     config \u001b[38;5;241m=\u001b[39m ckpt_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n","File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/qix/user_data/nlb/ndt_runs/mc_maze_small_from_scratch/mc_maze_small_from_scratch.lve.pth'"]}],"source":["\"\"\"\n","- At this point we should be able to train a basic model.\n","- In CLI, run a training call, replacing the appropriate paths\n","```\n","    ./scripts/train.sh mc_maze_small_from_scratch\n","\n","    OR\n","\n","    python ray_random.py -e ./configs/mc_maze_small_from_scratch.yaml\n","    (CLI overrides aren't available here, so make another config file)\n","```\n","- Once this is done training (~0.5hr for non-search), let's load the results...\n","\"\"\"\n","import os\n","import os.path as osp\n","from pathlib import Path\n","import sys\n","\n","# Add ndt src if not in path\n","module_path = osp.abspath(osp.join('..'))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","\n","import time\n","import h5py\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import torch\n","import torch.nn.functional as f\n","from torch.utils import data\n","\n","from nlb_tools.evaluation import evaluate\n","\n","from src.run import prepare_config\n","from src.runner import Runner\n","from src.dataset import SpikesDataset, DATASET_MODES\n","\n","from analyze_utils import init_by_ckpt\n","\n","variant = \"mc_maze_small_from_scratch\"\n","\n","# is_ray = True\n","is_ray = False\n","\n","if is_ray:\n","    ckpt_path = f\"/home/qix/user_data/nlb/ndt_runs/ray/{variant}/best_model/ckpts/{variant}.lve.pth\"\n","    runner, spikes, rates, heldout_spikes, forward_spikes = init_by_ckpt(ckpt_path, mode=DATASET_MODES.val)\n","else:\n","    ckpt_dir = Path(\"/home/qix/user_data/nlb/ndt_runs/\")\n","    ckpt_path = ckpt_dir / variant / f\"{variant}.lve.pth\"\n","    runner, spikes, rates, heldout_spikes, forward_spikes = init_by_ckpt(ckpt_path, mode=DATASET_MODES.val)\n","\n","eval_rates, _ = runner.get_rates(\n","    checkpoint_path=ckpt_path,\n","    save_path = None,\n","    mode = DATASET_MODES.val\n",")\n","train_rates, _ = runner.get_rates(\n","    checkpoint_path=ckpt_path,\n","    save_path = None,\n","    mode = DATASET_MODES.train\n",")\n","\n","# * Val\n","eval_rates, eval_rates_forward = torch.split(eval_rates, [spikes.size(1), eval_rates.size(1) - spikes.size(1)], 1)\n","eval_rates_heldin_forward, eval_rates_heldout_forward = torch.split(eval_rates_forward, [spikes.size(-1), heldout_spikes.size(-1)], -1)\n","train_rates, _ = torch.split(train_rates, [spikes.size(1), train_rates.size(1) - spikes.size(1)], 1)\n","eval_rates_heldin, eval_rates_heldout = torch.split(eval_rates, [spikes.size(-1), heldout_spikes.size(-1)], -1)\n","train_rates_heldin, train_rates_heldout = torch.split(train_rates, [spikes.size(-1), heldout_spikes.size(-1)], -1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# * Viz some trials\n","trials = [1, 2, 3]\n","neuron = 10\n","trial_rates = train_rates_heldout[trials, :, neuron].cpu()\n","trial_spikes = heldout_spikes[trials, :, neuron].cpu()\n","trial_time = heldout_spikes.size(1)\n","\n","spike_level = 0.05\n","f, axes = plt.subplots(2, figsize=(6, 4))\n","times = np.arange(0, trial_time * 0.05, 0.05)\n","for trial_index in range(trial_spikes.size(0)):\n","    spike_times, = np.where(trial_spikes[trial_index].numpy())\n","    spike_times = spike_times * 0.05\n","    axes[0].scatter(spike_times, spike_level * (trial_index + 1)*np.ones_like(spike_times), marker='|', label='Spikes', s=30)\n","    axes[1].plot(times, trial_rates[trial_index].exp())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Submission e.g. as in `basic_example.ipynb`\n","# Looks like this model is pretty terrible :/\n","output_dict = {\n","    dataset_name + suffix: {\n","        'train_rates_heldin': train_rates_heldin.cpu().numpy(),\n","        'train_rates_heldout': train_rates_heldout.cpu().numpy(),\n","        'eval_rates_heldin': eval_rates_heldin.cpu().numpy(),\n","        'eval_rates_heldout': eval_rates_heldout.cpu().numpy(),\n","        'eval_rates_heldin_forward': eval_rates_heldin_forward.cpu().numpy(),\n","        'eval_rates_heldout_forward': eval_rates_heldout_forward.cpu().numpy()\n","    }\n","}\n","\n","# Reset logging level to hide excessive info messages\n","logging.getLogger().setLevel(logging.WARNING)\n","\n","# If 'val' phase, make the target data\n","if phase == 'val':\n","    # Note that the RTT task is not well suited to trial averaging, so PSTHs are not made for it\n","    target_dict = make_eval_target_tensors(dataset, dataset_name=dataset_name, train_trial_split='train', eval_trial_split='val', include_psth=True, save_file=False)\n","\n","    # Demonstrate target_dict structure\n","    print(target_dict.keys())\n","    print(target_dict[dataset_name + suffix].keys())\n","\n","# Set logging level again\n","logging.getLogger().setLevel(logging.INFO)\n","\n","if phase == 'val':\n","    print(evaluate(target_dict, output_dict))\n","\n","# e.g. with targets to compare to\n","# target_dict = torch.load(f'/snel/home/joely/tmp/{variant}_target.pth')\n","# target_dict = np.load(f'/snel/home/joely/tmp/{variant}_target.npy', allow_pickle=True).item()\n","\n","# print(evaluate(target_dict, output_dict))\n","\n","# e.g. to upload to EvalAI\n","# with h5py.File('ndt_maze_preds.h5', 'w') as f:\n","#     group = f.create_group('mc_maze')\n","#     for key in output_dict['mc_maze']:\n","#         group.create_dataset(key, data=output_dict['mc_maze'][key])"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
